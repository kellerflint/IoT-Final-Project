{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the subject to work with (use the actual #, not the index)\n",
    "subject_number = 1\n",
    "\n",
    "# Set the percentage of subject data to use for train and test sets.\n",
    "subject_data_percentage = 0.1\n",
    "\n",
    "# Sequence Length (in seconds)\n",
    "sequence_length = 30\n",
    "\n",
    "# Set the predictive horizon (in seconds)\n",
    "predictive_horizon = 5\n",
    "\n",
    "assert subject_number > 0, \"Subject number must be greater than 0.\"\n",
    "assert subject_data_percentage > 0, \"Subject data percentage must be greater than 0.\"\n",
    "assert predictive_horizon > 0, \"Predictive horizon must be greater than 0.\"\n",
    "assert sequence_length > 0, \"Sequence length must be greater than 0.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "sequence_length = sequence_length * 100\n",
    "predictive_horizon = predictive_horizon * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files\n",
    "files = glob.glob('data/ProcessedData_Subject*.csv')\n",
    "\n",
    "data_dfs = []\n",
    "\n",
    "for filepath in files:\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Extract subject number from path\n",
    "    current_subject_number = filepath.split('/')[-1].split('_Subject')[1].split('.')[0].lstrip('0')\n",
    "    df['Subject Number'] = current_subject_number\n",
    "    data_dfs.append(df)\n",
    "    \n",
    "    if str(current_subject_number) == str(subject_number): # Early exit condition since we're only running this for 1 subject\n",
    "        break\n",
    "\n",
    "# Check\n",
    "#data_dfs[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = data_dfs[subject_number - 1]\n",
    "\n",
    "def remove_outliers(df, columns):\n",
    "    z_scores = np.abs(stats.zscore(df[columns]))\n",
    "    filtered_entries = (z_scores < 4).all(axis=1)  # Relaxing the threshold to 4\n",
    "    return df[filtered_entries]\n",
    "\n",
    "def standardize(df, columns):\n",
    "    scaler = StandardScaler()\n",
    "    df.loc[:, columns] = scaler.fit_transform(df[columns])\n",
    "    return df\n",
    "\n",
    "# Fill missing values with forward fill\n",
    "subject_df.ffill(inplace=True)\n",
    "\n",
    "# Automatically define columns to process by excluding 'Time [s]'\n",
    "columns_to_process = df.columns.drop(['Time [s]', 'Subject Number']).tolist()\n",
    "\n",
    "subject_df_clean = remove_outliers(df, columns_to_process)\n",
    "\n",
    "subject_df_standardized = standardize(subject_df_clean, columns_to_process)\n",
    "\n",
    "subject_df = subject_df_standardized\n",
    "\n",
    "# Check\n",
    "#subject_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Train / Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original subject size:  117578\n",
      "Subset size:  11757\n",
      "Train set size:  9405\n",
      "Test set size:  2352\n"
     ]
    }
   ],
   "source": [
    "size = int(len(subject_df) * subject_data_percentage)\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Ensure the random start does not make the slice exceed the DataFrame's length\n",
    "max_start_index = len(subject_df) - size\n",
    "\n",
    "# Choose a random start point (skipping the first 10 seconds of data)\n",
    "start_index = np.random.randint(1000, max_start_index)\n",
    "\n",
    "subset_df = subject_df.iloc[start_index:start_index + size]\n",
    "\n",
    "train_df, test_df = train_test_split(subset_df, test_size=0.2)\n",
    "\n",
    "train_df.reset_index()\n",
    "test_df.reset_index()\n",
    "\n",
    "# Checks\n",
    "print(\"Original subject size: \", len(subject_df))\n",
    "print(\"Subset size: \", len(subset_df))\n",
    "print(\"Train set size: \", len(train_df))\n",
    "print(\"Test set size: \", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5905, 3000, 9), (5905,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_arrays = []\n",
    "target_arrays = []\n",
    "\n",
    "feat_cols = columns_to_process # TODO: Not sure if we want to do predictions on all of these columns\n",
    "target_cols = ['Flow [L/s]']\n",
    "\n",
    "for i in range(0, len(train_df) - sequence_length - predictive_horizon):\n",
    "    sequence_arrays.append(train_df.iloc[i:i + sequence_length][columns_to_process].values)\n",
    "    target_arrays.append(train_df.iloc[i + sequence_length + predictive_horizon][target_cols].iloc[0])\n",
    "\n",
    "# Convert to numpy arrays and floats    \n",
    "sequence_arrays = np.array(sequence_arrays, dtype = object).astype(np.float32)\n",
    "target_arrays = np.array(target_arrays, dtype = object).astype(np.float32)\n",
    "\n",
    "# Check\n",
    "sequence_arrays.shape, target_arrays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you'd generate the spectrogram for each sequence / feature. \n",
    "# It'd be great if this could happen only when needed to make the next prediction \n",
    "# so that we don't have to store all the spectrograms in memory at once."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
