{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fbxQg7wZmf1"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HqRnDl5PZmf2"
      },
      "outputs": [],
      "source": [
        "# Set the subject to work with (use the actual #, not the index)\n",
        "subject_number = 1\n",
        "\n",
        "# Set the percentage of subject data to use for train and test sets.\n",
        "subject_data_percentage = 0.1\n",
        "\n",
        "# Sequence Length (in seconds)\n",
        "sequence_length = 30\n",
        "\n",
        "# Set the predictive horizon (in seconds)\n",
        "predictive_horizon = 5\n",
        "\n",
        "assert subject_number > 0, \"Subject number must be greater than 0.\"\n",
        "assert subject_data_percentage > 0, \"Subject data percentage must be greater than 0.\"\n",
        "assert predictive_horizon > 0, \"Predictive horizon must be greater than 0.\"\n",
        "assert sequence_length > 0, \"Sequence length must be greater than 0.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IayqVwRZmf3"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LVbx_P7ZZmf3"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5dSAfLE-Zmf3"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "sequence_length = sequence_length * 100\n",
        "predictive_horizon = predictive_horizon * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeZYfXZJZmf3"
      },
      "source": [
        "# Data Prep\n",
        "\n",
        "### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuKoQ60NaMEn",
        "outputId": "b16bfb20-95d3-4879-ddc2-f2a9e806b119"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K_UqMrumZmf3"
      },
      "outputs": [],
      "source": [
        "# Read in files\n",
        "files = glob.glob('/content/drive/MyDrive/Data/ProcessedData_Subject*.csv')\n",
        "\n",
        "data_dfs = []\n",
        "\n",
        "for filepath in files:\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # Extract subject number from path\n",
        "    current_subject_number = filepath.split('/')[-1].split('_Subject')[1].split('.')[0].lstrip('0')\n",
        "    df['Subject Number'] = current_subject_number\n",
        "    data_dfs.append(df)\n",
        "\n",
        "    if str(current_subject_number) == str(subject_number): # Early exit condition since we're only running this for 1 subject\n",
        "        break\n",
        "\n",
        "# Check\n",
        "#data_dfs[0].head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = ['Time [s]', 'Pressure [cmH2O]', 'Flow [L/s]', 'V_tidal [L]', 'Subject Number']\n",
        "\n",
        "# Create a new list to hold the DataFrames with only the selected columns\n",
        "selected_data_dfs = []\n",
        "\n",
        "for df in data_dfs:\n",
        "    # Select only the specified columns\n",
        "    selected_df = df[selected_columns]\n",
        "    selected_data_dfs.append(selected_df)\n",
        "\n",
        "# Check the first few rows of the first selected DataFrame\n",
        "print(selected_data_dfs[0].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJrymJ-4aBLQ",
        "outputId": "9eeba440-2b7e-46ff-c245-a183e60d1b89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Time [s]  Pressure [cmH2O]  Flow [L/s]  V_tidal [L] Subject Number\n",
            "0 -1.000000e-02         -3.400773    0.713827     0.299421              1\n",
            "1  2.275957e-15         -3.400773    0.713827     0.306559              1\n",
            "2  1.000000e-02         -3.282765    0.659553     0.313426              1\n",
            "3  2.000000e-02         -3.400773    0.739471     0.320421              1\n",
            "4  3.000000e-02         -3.325677    0.739471     0.327816              1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLLB6-jJZmf3"
      },
      "source": [
        "### Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh5FojF5Zmf3",
        "outputId": "6e774ede-691e-4505-f492-e134b34a05d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Time [s]  Pressure [cmH2O]  Flow [L/s]  V_tidal [L] Subject Number\n",
            "0 -1.000000e-02         -2.399169    0.879874    -0.774893              1\n",
            "1  2.275957e-15         -2.399169    0.879874    -0.770238              1\n",
            "2  1.000000e-02         -2.353036    0.794601    -0.765760              1\n",
            "3  2.000000e-02         -2.399169    0.920165    -0.761199              1\n",
            "4  3.000000e-02         -2.369812    0.920165    -0.756377              1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-9d0028a2c85c>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  subject_df.ffill(inplace=True)\n",
            "<ipython-input-7-9d0028a2c85c>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc[:, columns] = scaler.fit_transform(df[columns])\n"
          ]
        }
      ],
      "source": [
        "subject_df = selected_data_dfs[subject_number - 1]  # Assign the DataFrame for the chosen subject\n",
        "\n",
        "def remove_outliers(df, columns):\n",
        "    z_scores = np.abs(stats.zscore(df[columns]))\n",
        "    filtered_entries = (z_scores < 4).all(axis=1)  # Using a threshold of 4\n",
        "    return df[filtered_entries]\n",
        "\n",
        "def standardize(df, columns):\n",
        "    scaler = StandardScaler()\n",
        "    df.loc[:, columns] = scaler.fit_transform(df[columns])\n",
        "    return df\n",
        "\n",
        "# Fill missing values with forward fill\n",
        "subject_df.ffill(inplace=True)\n",
        "\n",
        "# Define columns to process, excluding 'Time [s]' and 'Subject Number'\n",
        "columns_to_process = subject_df.columns.drop(['Time [s]', 'Subject Number']).tolist()\n",
        "\n",
        "# Remove outliers and standardize\n",
        "subject_df_clean = remove_outliers(subject_df, columns_to_process)\n",
        "subject_df_standardized = standardize(subject_df_clean, columns_to_process)\n",
        "\n",
        "# Update subject_df with the processed DataFrame\n",
        "subject_df = subject_df_standardized\n",
        "\n",
        "# To check the first few rows of the processed DataFrame\n",
        "print(subject_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av59Jd4qZmf3"
      },
      "source": [
        "### Prepare Train / Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBMr1XcrZmf4",
        "outputId": "8f8a1385-123b-420f-f5d2-47069729519a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original subject size:  117960\n",
            "Subset size:  11796\n",
            "Train set size:  9436\n",
            "Test set size:  2360\n"
          ]
        }
      ],
      "source": [
        "size = int(len(subject_df) * subject_data_percentage)\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Ensure the random start does not make the slice exceed the DataFrame's length\n",
        "max_start_index = len(subject_df) - size\n",
        "\n",
        "# Choose a random start point (skipping the first 10 seconds of data)\n",
        "start_index = np.random.randint(1000, max_start_index)\n",
        "\n",
        "subset_df = subject_df.iloc[start_index:start_index + size]\n",
        "\n",
        "train_df, test_df = train_test_split(subset_df, test_size=0.2)\n",
        "\n",
        "train_df.reset_index()\n",
        "test_df.reset_index()\n",
        "\n",
        "# Checks\n",
        "print(\"Original subject size: \", len(subject_df))\n",
        "print(\"Subset size: \", len(subset_df))\n",
        "print(\"Train set size: \", len(train_df))\n",
        "print(\"Test set size: \", len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVmXJOMpZmf4"
      },
      "source": [
        "# Generate Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZurkJs7eZmf4",
        "outputId": "f9ba91b5-e409-41f4-eaa4-ed265d9ad7c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5936, 3000, 3), (5936,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "sequence_arrays = []\n",
        "target_arrays = []\n",
        "\n",
        "feat_cols = columns_to_process # TODO: Not sure if we want to do predictions on all of these columns\n",
        "target_cols = ['Flow [L/s]']\n",
        "\n",
        "for i in range(0, len(train_df) - sequence_length - predictive_horizon):\n",
        "    sequence_arrays.append(train_df.iloc[i:i + sequence_length][columns_to_process].values)\n",
        "    target_arrays.append(train_df.iloc[i + sequence_length + predictive_horizon][target_cols].iloc[0])\n",
        "\n",
        "# Convert to numpy arrays and floats\n",
        "sequence_arrays = np.array(sequence_arrays, dtype = object).astype(np.float32)\n",
        "target_arrays = np.array(target_arrays, dtype = object).astype(np.float32)\n",
        "\n",
        "# Check\n",
        "sequence_arrays.shape, target_arrays.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TxBxOH09Zmf4"
      },
      "outputs": [],
      "source": [
        "# Then you'd generate the spectrogram for each sequence / feature.\n",
        "# It'd be great if this could happen only when needed to make the next prediction\n",
        "# so that we don't have to store all the spectrograms in memory at once."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Spectrograms"
      ],
      "metadata": {
        "id": "b52imBoKhkni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import spectrogram\n",
        "\n",
        "fs = 100  # Sampling frequency\n",
        "\n",
        "output_directory = \"/content/drive/MyDrive/Data/Spectrograms\"\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "for seq_index, sequence in enumerate(sequence_arrays):\n",
        "    plt.figure(figsize=(14, 18))\n",
        "\n",
        "    for feature_index in range(sequence.shape[1]):\n",
        "        feature_sequence = sequence[:, feature_index]\n",
        "\n",
        "        # Computing the spectrogram for the current feature in the sequence\n",
        "        f, t, Sxx = spectrogram(feature_sequence, fs)\n",
        "\n",
        "        ax = plt.subplot(sequence.shape[1], 1, feature_index + 1)\n",
        "        plt.pcolormesh(t, f, 10 * np.log10(Sxx), shading='gouraud')\n",
        "        ax.set_ylabel('Frequency [Hz]')\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_ylim(0.5, 50)  # Adjust based on your data's frequency content\n",
        "        ax.set_xlabel('Time [s]')\n",
        "        ax.set_title(f'Spectrogram of Sequence {seq_index + 1}, Feature {feature_index + 1}')\n",
        "\n",
        "    plt.suptitle(f'Spectrograms for Sequence {seq_index + 1}', fontsize=16, y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Saving the figure with all feature spectrograms for the current sequence\n",
        "    image_name = f\"Spectrograms_Sequence_{seq_index + 1}.png\"\n",
        "    plt.savefig(os.path.join(output_directory, image_name))\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "YAQeqv42bbOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class SpectrogramDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, spectrogram_paths, labels, batch_size=32, img_size=(128, 128), n_channels=1, shuffle=True):\n",
        "        self.spectrogram_paths = spectrogram_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.spectrogram_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        spectrogram_paths_temp = [self.spectrogram_paths[k] for k in indexes]\n",
        "        X, y = self.__data_generation(spectrogram_paths_temp)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.spectrogram_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, spectrogram_paths_temp):\n",
        "        X = np.empty((self.batch_size, *self.img_size, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        for i, spectrogram_path in enumerate(spectrogram_paths_temp):\n",
        "            img = tf.keras.preprocessing.image.load_img(spectrogram_path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "            X[i,] = tf.keras.preprocessing.image.img_to_array(img) / 255.\n",
        "            y[i] = self.labels[spectrogram_path]\n",
        "\n",
        "        return X, tf.keras.utils.to_categorical(y, num_classes=len(np.unique(list(self.labels.values()))))\n"
      ],
      "metadata": {
        "id": "iATk_NBk0aYS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, TimeDistributed, Dropout\n",
        "\n",
        "def create_cnn_lstm_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape))\n",
        "    model.add(TimeDistributed(MaxPooling2D(2, 2)))\n",
        "    model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "7FQyeU9d0bW1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Assuming spectrograms are saved and labels are prepared\n",
        "output_directory = \"/content/drive/MyDrive/Data/Spectrograms\"\n",
        "\n",
        "# Prepare your data generator for training\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    output_directory,\n",
        "    target_size=(128, 128),  # Assuming spectrogram size of 128x128\n",
        "    color_mode='grayscale',\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # Assuming more than 2 classes\n",
        ")\n",
        "\n",
        "# CNN-LSTM Model\n",
        "model = Sequential([\n",
        "    TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(None, 128, 128, 1)),\n",
        "    TimeDistributed(MaxPooling2D(2, 2)),\n",
        "    TimeDistributed(Flatten()),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_generator, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "qYFE-o3W0iLT",
        "outputId": "bf1aeae7-07df-4977-9204-5aaa5181a1c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-efe5ff840657>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;34m\"Asked to retrieve element {idx}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;34m\"but the Sequence \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8ChvcIz0liu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}